{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Do this to enable importing modules\n",
    "src_path = os.path.join(os.path.abspath(\"\"), \"..\")\n",
    "sys.path.insert(0, src_path)\n",
    "\n",
    "env_path = os.path.join(src_path, \"feature_pipeline/.env\")\n",
    "\n",
    "load_dotenv(env_path)  # take environment variables from feature pipeline subfolder .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_pipeline.llm_components.prompt_templates import QueryExpansionTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.openai_functions import (\n",
    "    convert_pydantic_to_openai_function,\n",
    ")\n",
    "from langchain.prompts\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryMetadata(BaseModel):\n",
    "    \"\"\"Information to extract from the user query. \n",
    "    Dates should be transformed to yyyy-mm-dd and be relative to the given current date\"\"\"\n",
    "\n",
    "    currency: str = Field(description=\"The cryptocurrency mentioned in the query.\")\n",
    "    date: str = Field(description=\"date from the text in the format yyyy-mm-dd\")\n",
    "\n",
    "\n",
    "openai_functions = [convert_pydantic_to_openai_function(QueryMetadata)]\n",
    "openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'currency': 'btc', 'date': '2024-05-11'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(False)\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().strftime(format=\"%Y-%m-%d\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Today´s date is {current_date}.\"),\n",
    "    (\"human\", \"{user_query}\"),\n",
    "])\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "parser = JsonOutputFunctionsParser()\n",
    "chain = template | model.bind(functions=openai_functions) | parser\n",
    "\n",
    "query = \"What was the price of btc 10 days ago?\"\n",
    "chain.invoke({\"current_date\":current_date,  \"user_query\": query}, config={\"verbose\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing with functions\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Test the prompts with a small sample\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"Today´s date is {current_date}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What date was 5 days ago?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    functions=openai_functions\n",
    ")\n",
    "response.choices[0].message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering with extracted metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self._qdrant_client.search(\n",
    "      collection_name=\"vector_posts\",\n",
    "      query_filter=models.Filter(\n",
    "          must=[\n",
    "              models.FieldCondition(\n",
    "                  key=\"author_id\",\n",
    "                  match=models.MatchValue(\n",
    "                      value=metadata_filter_value,\n",
    "                  ),\n",
    "              )\n",
    "          ]\n",
    "      ),\n",
    "      query_vector=self._embedder.encode(generated_query).tolist(),\n",
    "      limit=k,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "import feature_pipeline.utils\n",
    "from qdrant_client import QdrantClient, models\n",
    "from feature_pipeline.rag.query_expanison import QueryExpansion\n",
    "from feature_pipeline.rag.reranking import Reranker\n",
    "from feature_pipeline.rag.self_query import SelfQuery\n",
    "from sentence_transformers.SentenceTransformer import SentenceTransformer\n",
    "from feature_pipeline.settings import settings\n",
    "\n",
    "import feature_pipeline.logger_utils as logger_utils\n",
    "\n",
    "\n",
    "logger = logger_utils.get_logger(__name__)\n",
    "\n",
    "\n",
    "class VectorRetriever:\n",
    "    \"\"\"\n",
    "    Class for retrieving vectors from a Vector store in a RAG system using query expansion and Multitenancy search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, query: str):\n",
    "        self._client = QdrantClient(\n",
    "            host=settings.QDRANT_DATABASE_HOST, port=settings.QDRANT_DATABASE_PORT\n",
    "        )\n",
    "        self.query = query\n",
    "        self._embedder = SentenceTransformer(settings.EMBEDDING_MODEL_ID)\n",
    "        self._query_expander = QueryExpansion()\n",
    "        self._metadata_extractor = SelfQuery()\n",
    "        self._reranker = Reranker()\n",
    "\n",
    "    def _search_single_query(\n",
    "        self, generated_query: str, metadata_filter_value: str, k: int\n",
    "    ):\n",
    "        assert k > 3, \"k should be greater than 3\"\n",
    "\n",
    "        query_vector = self._embedder.encode(generated_query).tolist()\n",
    "        vectors = [\n",
    "            self._client.search(\n",
    "                collection_name=\"vector_posts\",\n",
    "                query_filter=models.Filter(\n",
    "                    must=[\n",
    "                        models.FieldCondition(\n",
    "                            key=\"author_id\",\n",
    "                            match=models.MatchValue(\n",
    "                                value=metadata_filter_value,\n",
    "                            ),\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "                query_vector=query_vector,\n",
    "                limit=k // 3,\n",
    "            ),\n",
    "            self._client.search(\n",
    "                collection_name=\"vector_articles\",\n",
    "                query_filter=models.Filter(\n",
    "                    must=[\n",
    "                        models.FieldCondition(\n",
    "                            key=\"author_id\",\n",
    "                            match=models.MatchValue(\n",
    "                                value=metadata_filter_value,\n",
    "                            ),\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "                query_vector=query_vector,\n",
    "                limit=k // 3,\n",
    "            ),\n",
    "            self._client.search(\n",
    "                collection_name=\"vector_repositories\",\n",
    "                query_filter=models.Filter(\n",
    "                    must=[\n",
    "                        models.FieldCondition(\n",
    "                            key=\"owner_id\",\n",
    "                            match=models.MatchValue(\n",
    "                                value=metadata_filter_value,\n",
    "                            ),\n",
    "                        )\n",
    "                    ]\n",
    "                ),\n",
    "                query_vector=query_vector,\n",
    "                limit=k // 3,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        return utils.flatten(vectors)\n",
    "\n",
    "    def retrieve_top_k(self, k: int, to_expand_to_n_queries: int) -> list:\n",
    "        generated_queries = self._query_expander.generate_response(\n",
    "            self.query, to_expand_to_n=to_expand_to_n_queries\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Successfully generated queries for search.\",\n",
    "            num_queries=len(generated_queries),\n",
    "        )\n",
    "\n",
    "        author_id = self._metadata_extractor.generate_response(self.query)\n",
    "        logger.info(\n",
    "            \"Successfully extracted the author_id from the query.\",\n",
    "            author_id=author_id,\n",
    "        )\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            search_tasks = [\n",
    "                executor.submit(\n",
    "                    self._search_single_query, query, author_id, k\n",
    "                )\n",
    "                for query in generated_queries\n",
    "            ]\n",
    "\n",
    "            hits = [\n",
    "                task.result() for task in concurrent.futures.as_completed(search_tasks)\n",
    "            ]\n",
    "            hits = utils.flatten(hits)\n",
    "\n",
    "        logger.info(\"All documents retrieved successfully.\", num_documents=len(hits))\n",
    "\n",
    "        return hits\n",
    "\n",
    "    def rerank(self, hits: list, keep_top_k: int) -> list[str]:\n",
    "        content_list = [hit.payload[\"content\"] for hit in hits]\n",
    "        rerank_hits = self._reranker.generate_response(\n",
    "            query=self.query, passages=content_list, keep_top_k=keep_top_k\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Documents reranked successfully.\", num_documents=len(rerank_hits))\n",
    "\n",
    "        return rerank_hits\n",
    "\n",
    "    def set_query(self, query: str):\n",
    "        self.query = query\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-system-_6t_t_f9-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
